{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv3D, MaxPooling3D, UpSampling3D, merge\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import SpatialDropout3D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import load_model\n",
    "\n",
    "from batchgenerators.dataloading.data_loader import DataLoader\n",
    "from batchviewer import view_batch\n",
    "from batchgenerators.augmentations.crop_and_pad_augmentations import crop\n",
    "from batchgenerators.augmentations.utils import pad_nd_image\n",
    "from batchgenerators.utilities.data_splitting import get_split_deterministic\n",
    "from batchgenerators.dataloading import MultiThreadedAugmenter\n",
    "from batchgenerators.transforms.spatial_transforms import SpatialTransform_2, MirrorTransform\n",
    "from batchgenerators.transforms.color_transforms import BrightnessMultiplicativeTransform, GammaTransform\n",
    "from batchgenerators.transforms.noise_transforms import GaussianNoiseTransform, GaussianBlurTransform\n",
    "from batchgenerators.transforms import Compose\n",
    "\n",
    "#Numbers below gets divided everytime we apply maxpooling; this can create issue when concatenating two models\n",
    "# NUM_SLIDES=slice_size\n",
    "# IMG_HEIGHT=slice_size\n",
    "# IMG_WIDTH=slice_size\n",
    "# IMG_CHANNELS=1\n",
    "datadir_path = \"./data/05dzgcM/\"\n",
    "name = '05dzgcM'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader3D_jfr(DataLoader):\n",
    "    def __init__(self, data, batch_size, patch_size, num_threads_in_multithreaded, seed_for_shuffle=42,\n",
    "                 return_incomplete=False, shuffle=True, infinite=True):\n",
    "        \"\"\"\n",
    "        data must be a list of patients as returned by get_list_of_patients (and split by get_split_deterministic)\n",
    "\n",
    "        patch_size is the spatial size the retured batch will have\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(data, batch_size, num_threads_in_multithreaded, seed_for_shuffle, return_incomplete, shuffle,\n",
    "                         infinite)\n",
    "        self.patch_size = patch_size\n",
    "        self.num_modalities = 0\n",
    "        self.indices = list(range(len(data)))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_patient(patient):\n",
    "        data = np.load(os.path.join(datadir_path,name+'_clean.npy'), mmap_mode=\"r\")\n",
    "#         data = data[:][np.newaxis]\n",
    "#         data = np.load(patient + \".npy\", mmap_mode=\"r\")\n",
    "        metadata = []\n",
    "#         metadata = load_pickle(patient + \".pkl\")\n",
    "        return data, metadata\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        # DataLoader has its own methods for selecting what patients to use next, see its Documentation\n",
    "        idx = self.get_indices()\n",
    "        patients_for_batch = [self._data[i] for i in idx]\n",
    "\n",
    "        # initialize empty array for data and seg\n",
    "        data = np.zeros((self.batch_size, self.num_modalities, *self.patch_size), dtype=np.float32)\n",
    "        seg = np.zeros((self.batch_size, 1, *self.patch_size), dtype=np.float32)\n",
    "\n",
    "        metadata = []\n",
    "        patient_names = []\n",
    "\n",
    "        # iterate over patients_for_batch and include them in the batch\n",
    "        for i, j in enumerate(patients_for_batch):\n",
    "            patient_data, patient_metadata = self.load_patient(j)\n",
    "            \n",
    "            # this will only pad patient_data if its shape is smaller than self.patch_size\n",
    "            patient_data = pad_nd_image(patient_data, self.patch_size)\n",
    "\n",
    "            # now random crop to self.patch_size\n",
    "            # crop expects the data to be (b, c, x, y, z) but patient_data is (c, x, y, z) so we need to add one\n",
    "            # dummy dimension in order for it to work (@Todo, could be improved)\n",
    "            patient_data, patient_seg = crop(patient_data[:-1][None], \n",
    "                                             patient_data[-1:][None], \n",
    "                                             self.patch_size, \n",
    "                                             crop_type=\"random\")\n",
    "\n",
    "            data[i] = patient_data[0]\n",
    "            seg[i] = patient_seg[0]\n",
    "\n",
    "            metadata.append(patient_metadata)\n",
    "            patient_names.append(j)\n",
    "\n",
    "        return {'data': data, 'seg':seg, 'metadata':metadata, 'names':patient_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform(patch_size):\n",
    "    # we now create a list of transforms. These are not necessarily the best transforms to use for BraTS, this is just\n",
    "    # to showcase some things\n",
    "    tr_transforms = []\n",
    "\n",
    "    # the first thing we want to run is the SpatialTransform. It reduces the size of our data to patch_size and thus\n",
    "    # also reduces the computational cost of all subsequent operations. All subsequent operations do not modify the\n",
    "    # shape and do not transform spatially, so no border artifacts will be introduced\n",
    "    # Here we use the new SpatialTransform_2 which uses a new way of parameterizing elastic_deform\n",
    "    # We use all spatial transformations with a probability of 0.2 per sample. This means that 1 - (1 - 0.1) ** 3 = 27%\n",
    "    # of samples will be augmented, the rest will just be cropped\n",
    "    tr_transforms.append(\n",
    "        SpatialTransform_2(\n",
    "            patch_size, [i // 2 for i in patch_size],\n",
    "            do_elastic_deform=True, deformation_scale=(0, 0.25),\n",
    "            do_rotation=True,\n",
    "            angle_x=(- 15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
    "            angle_y=(- 15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
    "            angle_z=(- 15 / 360. * 2 * np.pi, 15 / 360. * 2 * np.pi),\n",
    "            do_scale=True, scale=(0.75, 1.25),\n",
    "            border_mode_data='constant', border_cval_data=0,\n",
    "            border_mode_seg='constant', border_cval_seg=0,\n",
    "            order_seg=1, order_data=3,\n",
    "            random_crop=True,\n",
    "            p_el_per_sample=0.1, p_rot_per_sample=0.1, p_scale_per_sample=0.1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # now we mirror along all axes\n",
    "    tr_transforms.append(MirrorTransform(axes=(0, 1, 2)))\n",
    "\n",
    "    # brightness transform for 15% of samples\n",
    "    tr_transforms.append(BrightnessMultiplicativeTransform((0.7, 1.5), per_channel=True, p_per_sample=0.15))\n",
    "\n",
    "    # gamma transform. This is a nonlinear transformation of intensity values\n",
    "    # (https://en.wikipedia.org/wiki/Gamma_correction)\n",
    "    tr_transforms.append(GammaTransform(gamma_range=(0.5, 2), invert_image=False, per_channel=True, p_per_sample=0.15))\n",
    "    # we can also invert the image, apply the transform and then invert back\n",
    "    tr_transforms.append(GammaTransform(gamma_range=(0.5, 2), invert_image=True, per_channel=True, p_per_sample=0.15))\n",
    "\n",
    "    # Gaussian Noise\n",
    "    tr_transforms.append(GaussianNoiseTransform(noise_variance=(0, 0.05), p_per_sample=0.15))\n",
    "\n",
    "    # blurring. Some BraTS cases have very blurry modalities. This can simulate more patients with this problem and\n",
    "    # thus make the model more robust to it\n",
    "    tr_transforms.append(GaussianBlurTransform(blur_sigma=(0.5, 1.5), different_sigma_per_channel=True,\n",
    "                                               p_per_channel=0.5, p_per_sample=0.15))\n",
    "\n",
    "    # now we compose these transforms together\n",
    "    tr_transforms = Compose(tr_transforms)\n",
    "    return tr_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = [1]\n",
    "patients=[1]\n",
    "num_threads_for_brats_example = 8\n",
    "\n",
    "# train, val = get_split_deterministic(patients, fold=0, num_splits=5, random_state=42)\n",
    "patch_size = (128, 128, 128)\n",
    "batch_size = 2\n",
    "dataloader = DataLoader3D_jfr(train, batch_size, patch_size, num_threads_in_multithreaded=1)\n",
    "batch = next(dataloader)\n",
    "# batch viewer can show up to 4d tensors. We can show only one sample, but that should be sufficient here\n",
    "# view_batch(batch['data'][0], batch['seg'][0])\n",
    "\n",
    "# first let's collect all shapes, you will see why later\n",
    "shapes = [DataLoader3D_jfr.load_patient(i)[0].shape[1:] for i in patients]\n",
    "max_shape = np.max(shapes, 0)\n",
    "max_shape = np.max((max_shape, patch_size), 0)\n",
    "\n",
    "# we create a new instance of DataLoader. This one will return batches of shape max_shape. Cropping/padding is\n",
    "# now done by SpatialTransform. If we do it this way we avoid border artifacts (the entire brain of all cases will\n",
    "# be in the batch and SpatialTransform will use zeros which is exactly what we have outside the brain)\n",
    "# this is viable here but not viable if you work with different data. If you work for example with CT scans that\n",
    "# can be up to 500x500x500 voxels large then you should do this differently. There, instead of using max_shape you\n",
    "# should estimate what shape you need to extract so that subsequent SpatialTransform does not introduce border\n",
    "# artifacts\n",
    "dataloader_train = DataLoader3D_jfr(train, batch_size, max_shape, num_threads_for_brats_example)\n",
    "\n",
    "# during training I like to run a validation from time to time to see where I am standing. This is not a correct\n",
    "# validation because just like training this is patch-based but it's good enough. We don't do augmentation for the\n",
    "# validation, so patch_size is used as shape target here\n",
    "# dataloader_validation = DataLoader3D_jfr(val, batch_size, patch_size, max(1, num_threads_for_brats_example // 2))\n",
    "\n",
    "tr_transforms = get_train_transform(patch_size)\n",
    "\n",
    "# finally we can create multithreaded transforms that we can actually use for training\n",
    "# we don't pin memory here because this is pytorch specific.\n",
    "tr_gen = MultiThreadedAugmenter(dataloader_train, tr_transforms, num_processes=num_threads_for_brats_example,\n",
    "                                num_cached_per_queue=3,\n",
    "                                seeds=None, pin_memory=False)\n",
    "#     # we need less processes for vlaidation because we dont apply transformations\n",
    "#     val_gen = MultiThreadedAugmenter(dataloader_validation, None,\n",
    "#                                      num_processes=max(1, num_threads_for_brats_example // 2), num_cached_per_queue=1,\n",
    "#                                      seeds=None,\n",
    "#                                      pin_memory=False)\n",
    "\n",
    "#     # lets start the MultiThreadedAugmenter. This is not necessary but allows them to start generating training\n",
    "#     # batches while other things run in the main thread\n",
    "#     tr_gen.restart()\n",
    "#     val_gen.restart()\n",
    "\n",
    "#     # now if this was a network training you would run epochs like this (remember tr_gen and val_gen generate\n",
    "#     # inifinite examples! Don't do \"for batch in tr_gen:\"!!!):\n",
    "#     num_batches_per_epoch = 10\n",
    "#     num_validation_batches_per_epoch = 3\n",
    "#     num_epochs = 5\n",
    "#     # let's run this to get a time on how long it takes\n",
    "#     time_per_epoch = []\n",
    "#     start = time()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         start_epoch = time()\n",
    "#         for b in range(num_batches_per_epoch):\n",
    "#             batch = next(tr_gen)\n",
    "#             # do network training here with this batch\n",
    "\n",
    "#         for b in range(num_validation_batches_per_epoch):\n",
    "#             batch = next(val_gen)\n",
    "#             # run validation here\n",
    "#         end_epoch = time()\n",
    "#         time_per_epoch.append(end_epoch - start_epoch)\n",
    "#     end = time()\n",
    "#     total_time = end - start\n",
    "#     print(\"Running %d epochs took a total of %.2f seconds with time per epoch being %s\" %\n",
    "#           (num_epochs, total_time, str(time_per_epoch)))\n",
    "\n",
    "# if you notice that you have CPU usage issues, reduce the probability with which the spatial transformations are\n",
    "# applied in get_train_transform (down to 0.1 for example). SpatialTransform is the most expensive transform\n",
    "\n",
    "# if you wish to visualize some augmented examples, install batchviewer and uncomment this\n",
    "if view_batch is not None:\n",
    "    for _ in range(4):\n",
    "        batch = next(tr_gen)\n",
    "        view_batch(batch['data'][0], batch['seg'][0])\n",
    "else:\n",
    "    print(\"Cannot visualize batches, install batchviewer first. It's a nice and handy tool. You can get it here: \"\n",
    "          \"https://github.com/FabianIsensee/BatchViewer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build U-Net model\n",
    "inputs = keras.layers.Input((NUM_SLIDES,IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "#s = keras.layers.Lambda(lambda x: x / 3095)(inputs)\n",
    " \n",
    "c1 = keras.layers.Conv3D(16, kernel_size=(3,3,3), activation='relu',padding='same')(inputs)\n",
    "c1 = keras.layers.SpatialDropout3D(0.3)(c1)\n",
    "c1 = keras.layers.Conv3D(16, (3,3,3), activation='relu',padding='same')(c1)\n",
    "#GlobalAveragePooling3D -- try this as well\n",
    "p1 = keras.layers.MaxPooling3D((2,2,2))(c1)\n",
    "#p1 = BatchNormalization()(p1)\n",
    " \n",
    "c2 = keras.layers.Conv3D(32, (3,3,3), activation='relu',padding='same')(p1)\n",
    "c2 = keras.layers.SpatialDropout3D(0.3)(c2)\n",
    "c2 = keras.layers.Conv3D(32, (3,3,3), activation='relu',padding='same')(c2)\n",
    "p2 = keras.layers.MaxPooling3D((2,2,2))(c2)\n",
    "#p2 = BatchNormalization()(p2)\n",
    "\n",
    "c3 = keras.layers.Conv3D(64, (3,3,3), activation='relu',padding='same')(p2)\n",
    "c3 = keras.layers.SpatialDropout3D(0.3)(c3)\n",
    "c3 = keras.layers.Conv3D(64, (3,3,3), activation='relu',padding='same')(c3)\n",
    "p3 = keras.layers.MaxPooling3D((2, 2,2))(c3)\n",
    "#p3 = BatchNormalization()(p3)\n",
    "\n",
    "c4 = keras.layers.Conv3D(128, (3,3,3), activation='relu',padding='same')(p3)\n",
    "c4 = keras.layers.SpatialDropout3D(0.3)(c4)\n",
    "c4 = keras.layers.Conv3D(128, (3,3,3), activation='relu', padding='same')(c4)\n",
    "p4 = keras.layers.MaxPooling3D(pool_size=(2,2,2))(c4)\n",
    "#p4 = BatchNormalization()(p4)\n",
    "\n",
    "c5 = keras.layers.Conv3D(256, (3,3,3), activation='relu',padding='same')(p4)\n",
    "c5 = keras.layers.SpatialDropout3D(0.3)(c5)\n",
    "c5 = keras.layers.Conv3D(256, (3,3,3), activation='relu', padding='same')(c5)\n",
    "p5 = keras.layers.MaxPooling3D(pool_size=(2,2,2))(c5)\n",
    "#p5 = BatchNormalization()(p5)\n",
    "\n",
    "\n",
    "c55 = keras.layers.Conv3D(512, (3,3,3), activation='relu',padding='same')(p5)\n",
    "c55 = keras.layers.SpatialDropout3D(0.3)(c55)\n",
    "c55 = keras.layers.Conv3D(512, (3,3,3), activation='relu',padding='same')(c55)\n",
    "#c55 = BatchNormalization()(c55)\n",
    "\n",
    "\n",
    "u66 = keras.layers.Conv3DTranspose(256, (2,2,2), strides=(2,2,2), padding='same')(c55)\n",
    "u66 = keras.layers.concatenate([u66, c5])\n",
    "c66 = keras.layers.Conv3D(256, (3,3,3), activation='relu',padding='same')(u66)\n",
    "c66 = keras.layers.SpatialDropout3D(0.3)(c66)\n",
    "c66 = keras.layers.Conv3D(256, (3,3,3), activation='relu',padding='same')(c66)\n",
    "#c66 = BatchNormalization()(c66)\n",
    "\n",
    "u6 = keras.layers.Conv3DTranspose(128, (2,2,2), strides=(2,2,2), padding='same')(c66)\n",
    "u6 = keras.layers.concatenate([u6, c4])\n",
    "c6 = keras.layers.Conv3D(128, (3,3,3), activation='relu',padding='same')(u6)\n",
    "c6 = keras.layers.SpatialDropout3D(0.3)(c6)\n",
    "c6 = keras.layers.Conv3D(128, (3,3,3), activation='relu',padding='same')(c6)\n",
    "#c6 = BatchNormalization()(c6)\n",
    "\n",
    "u7 = keras.layers.Conv3DTranspose(64, (2, 2,2), strides=(2, 2,2), padding='same')(c6)\n",
    "u7 = keras.layers.concatenate([u7, c3])\n",
    "c7 = keras.layers.Conv3D(64, (3, 3,3), activation='relu',padding='same')(u7)\n",
    "c7 = keras.layers.SpatialDropout3D(0.3)(c7)\n",
    "c7 = keras.layers.Conv3D(64, (3, 3,3), activation='relu',padding='same')(c7)\n",
    "#c7 = BatchNormalization()(c7)\n",
    "\n",
    "u8 = keras.layers.Conv3DTranspose(32, (2, 2,2), strides=(2, 2,2), padding='same')(c7)\n",
    "u8 = keras.layers.concatenate([u8, c2])\n",
    "c8 = keras.layers.Conv3D(32, (3, 3,3), activation='relu',padding='same')(u8)\n",
    "c8 = keras.layers.SpatialDropout3D(0.3)(c8)\n",
    "c8 = keras.layers.Conv3D(32, (3, 3,3), activation='relu',padding='same')(c8)\n",
    "#c8 = BatchNormalization()(c8)\n",
    "\n",
    "u9 = keras.layers.Conv3DTranspose(16, (2, 2,2), strides=(2, 2,2), padding='same')(c8)\n",
    "u9 = keras.layers.concatenate([u9, c1], axis=4)\n",
    "c9 = keras.layers.Conv3D(16, (3, 3,3), activation='relu',padding='same')(u9)\n",
    "c9 = keras.layers.SpatialDropout3D(0.3)(c9)\n",
    "c9 = keras.layers.Conv3D(16, (3, 3,3), activation='relu',padding='same')(c9)\n",
    "#c9 = BatchNormalization()(c9)\n",
    " \n",
    "outputs = keras.layers.Conv3D(1, (1, 1,1), activation='sigmoid')(c9)\n",
    " \n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef,recall_metric])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
